name: charmed-spark
summary: Spark ROCK
description: Spark ROCK
license: Apache-2.0

version: "3.3.2"
base: ubuntu:22.04
platforms:
  amd64:

services:
  spark:
    override: replace
    command: /opt/spark/pebble-start.sh
    startup: enabled
    environment:
      SPARK_HOME: /opt/spark
      JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64
      PYTHONPATH: /opt/spark/python:/opt/spark/python/build:/opt/spark/bin:$SNAP/usr/lib/python3/dist-packages:$PYTHONPATH

parts:

  spark:
    plugin: dump
    source: https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz
    source-checksum: sha512/4cd2396069fbe0f8efde2af4fd301bf46f8c6317e9dea1dd42a405de6a38380635d49b17972cb92c619431acece2c3af4c23bfdf193cedb3ea913ed69ded23a1
    overlay-script: |
      set -ex
      sed -i 's/http:\/\/deb.\(.*\)/https:\/\/deb.\1/g' /etc/apt/sources.list
      apt-get update
      ln -svf /lib /lib64
      # apt-get install -y bash tini libc6 libpam-modules krb5-user libnss3 procps
      apt-get install -y bash
      mkdir -p /opt/spark/python
      touch /opt/spark/RELEASE
      rm /bin/sh
      ln -svf /bin/bash /bin/sh
      echo "auth required pam_wheel.so use_uid" >> /etc/pam.d/su
      # chgrp root /etc/passwd && chmod ug+rw /etc/passwd
      rm -rf /var/cache/apt/*
    organize:
      conf: opt/spark/conf
      jars: opt/spark/jars
      bin: opt/spark/bin
      sbin: opt/spark/sbin
      python: opt/spark/python
      kubernetes/dockerfiles/spark/entrypoint.sh: opt/spark/entrypoint.sh
      kubernetes/dockerfiles/spark/decom.sh: opt/spark/decom.sh
      examples: opt/spark/examples
      kubernetes/tests: opt/spark/tests
      data: opt/spark/data
    stage:
       - opt/spark/conf
       - opt/spark/jars
       - opt/spark/bin
       - opt/spark/sbin
       - opt/spark/entrypoint.sh
       - opt/spark/decom.sh
       - opt/spark/examples
       - opt/spark/tests
       - opt/spark/data
       - opt/spark/python

  hadoop-jars:
    plugin: nil
    after: [spark]
    build-packages:
      - wget
    overlay-script: |
      AWS_JAVA_SDK_BUNDLE_VERSION='1.11.874'
      HADOOP_AWS_VERSION='3.2.2'
      mkdir -p $CRAFT_PART_INSTALL/opt/spark/jars
      cd $CRAFT_PART_INSTALL/opt/spark/jars
      wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar"
      wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar.sha1"  
      echo "`cat aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar.sha1`  aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar"
      wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar.sha1"
      echo "`cat hadoop-aws-${HADOOP_AWS_VERSION}.jar.sha1`  hadoop-aws-${HADOOP_AWS_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: hadoop-aws-${HADOOP_AWS_VERSION}.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
    stage:
      - opt/spark/jars

  client-scripts:
    plugin: nil
    after: [hadoop-jars]
    build-packages:
      - wget
      - ssl-cert
    overlay-packages:
      - python3-pip
    overlay-script: |
      SPARK_CLIENT_PKG_VERSION='0.0.7'
      mkdir -p $CRAFT_PART_INSTALL/opt/spark/python/dist
      cd $CRAFT_PART_INSTALL/opt/spark/python/dist
      echo "Downloading latest available Spark wheel package release v${SPARK_CLIENT_PKG_VERSION}."
      wget -q https://github.com/canonical/spark-client-snap/releases/download/v${SPARK_CLIENT_PKG_VERSION}/spark_client-${SPARK_CLIENT_PKG_VERSION}-py3-none-any.whl
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: Spark client package v${SPARK_CLIENT_PKG_VERSION} could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      pip install --target=${CRAFT_PART_INSTALL}/opt/spark/python/dist ./spark_client-${SPARK_CLIENT_PKG_VERSION}-py3-none-any.whl
    stage:
      - opt/spark/python/dist

  spark-client-apps:
    plugin: dump
    after: [ client-scripts ]
    source: files/spark/bin
    organize:
      spark-client.pyspark: opt/spark/python/bin/spark-client.pyspark
      spark-client.service-account-registry: opt/spark/python/bin/spark-client.service-account-registry
      spark-client.spark-shell: opt/spark/python/bin/spark-client.spark-shell
      spark-client.spark-submit: opt/spark/python/bin/spark-client.spark-submit
    stage:
      - opt/spark/python/bin/spark-client.pyspark
      - opt/spark/python/bin/spark-client.service-account-registry
      - opt/spark/python/bin/spark-client.spark-shell
      - opt/spark/python/bin/spark-client.spark-submit

  kubectl-bin:
    plugin: nil
    after: [client-scripts]
    build-packages:
      - wget
    overlay-script: |
      mkdir -p $CRAFT_PART_INSTALL/usr/local/bin
      cd $CRAFT_PART_INSTALL/usr/local/bin
      KUBECTL_VERSION=$(wget -qO- https://dl.k8s.io/release/stable.txt)
      wget -q "https://dl.k8s.io/release/$KUBECTL_VERSION/bin/linux/amd64/kubectl"
      wget -q  "https://dl.k8s.io/$KUBECTL_VERSION/bin/linux/amd64/kubectl.sha256"
      echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: kubectl could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      chmod +x kubectl
    stage:
      - usr/local/bin

  spark-conf:
    plugin: dump
    after: [kubectl-bin]
    source: files/spark/conf
    organize:
      spark-defaults.conf: etc/spark/spark-defaults.conf
    stage:
      - etc/spark/spark-defaults.conf

  user-setup:
    plugin: nil
    after: [spark-conf]
    overlay-packages:
      - tini
      - libc6
      - libpam-modules
      - krb5-user
      - libnss3
      - procps
      - openjdk-11-jre-headless
      - python3-setuptools
    overlay-script: |
      SPARK_GID=185
      SPARK_UID=185
      
      # Create a user in the $CRAFT_OVERLAY chroot
      groupadd -R $CRAFT_OVERLAY -g ${SPARK_GID} spark
      useradd -R $CRAFT_OVERLAY -m -r -g spark -u ${SPARK_UID} spark
    override-prime: |  
      SPARK_GID=185
      SPARK_UID=185
      
      craftctl default
      chown -R ${SPARK_GID}:${SPARK_UID} opt/spark
      chmod -R 770 opt/spark
      
      chown ${SPARK_GID}:${SPARK_UID} opt/spark/entrypoint.sh
      chmod 770 opt/spark/entrypoint.sh
      
      chown ${SPARK_GID}:${SPARK_UID} opt/spark/decom.sh
      chmod 770 opt/spark/decom.sh

  pebble-setup:
    plugin: dump
    source: files/pebble
    organize:
      pebble-start.sh: opt/spark/pebble-start.sh
    overlay-script:
      mkdir -p $CRAFT_PART_INSTALL/var/lib/pebble/default      
      mkdir -p $CRAFT_PART_INSTALL/var/lib/spark
      mkdir -p $CRAFT_PART_INSTALL/var/log/spark
      mkdir -p $CRAFT_PART_INSTALL/etc/spark
    stage:
      - opt/spark/pebble-start.sh
      - var/lib/pebble/default
      - var/lib/spark
      - var/log/spark
      - etc/spark
