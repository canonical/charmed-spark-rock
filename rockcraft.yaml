name: charmed-spark
summary: Charmed Apache Spark ROCK
description: |
  This is an OCI image that bundles Apache Spark binaries together with other
  tools of its ecosystem in order to be used in Charmed Operators, providing
  an automated and seamless experience to deploy, operate, manage and monitor
  SparkJob on K8s cluster.
  It is an open source, end-to-end, production ready data platform on top of
  cloud native technologies.

license: Apache-2.0

version: "3.4.2"
# version:spark:3.4.2
# version:jupyter:4.0.11

base: ubuntu@22.04

platforms:
  amd64:

run_user: _daemon_

environment:
  SPARK_HOME: /opt/spark
  SPARK_CONFS: /etc/spark8t/conf
  JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64
  PYTHONPATH: /opt/spark/python:/opt/spark8t/python/dist:/usr/lib/python3.10/site-packages
  PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/spark:/opt/spark/bin:/opt/spark/python/bin:/opt/spark-client/python/bin
  HOME: /var/lib/spark
  SPARK_USER_DATA: /var/lib/spark
  SPARK_LOG_DIR: /var/log/spark

services:
  sparkd:
    command: "/bin/bash /opt/pebble/sparkd.sh"
    summary: "This is the service to startup Spark processes using the Spark entrypoint"
    override: replace
    startup: enabled
    on-success: shutdown
    on-failure: shutdown
  history-server:
    command: "/bin/bash /opt/pebble/history-server.sh"
    summary: "This is the Spark History Server service"
    override: replace
    startup: disabled
    # Not working in charms
    # working-dir: /opt/spark
    environment:
      SPARK_PROPERTIES_FILE: /etc/spark8t/conf/spark-defaults.conf
  kyuubi:
    command: "/bin/bash /opt/pebble/kyuubi.sh"
    summary: "This is the Kyuubi service"
    override: replace
    startup: disabled

parts:
  spark:
    plugin: dump
    source: https://github.com/canonical/central-uploader/releases/download/spark-3.4.2-ubuntu1/spark-3.4.2-ubuntu1-20231214143421-bin-k8s.tgz
    source-checksum: sha512/a111897557921c4dd61daa67fb6979e0a231d4c5f56a0007f26663cb69ab42a12beb83e46edd8961baa7e7b62f031801c1b82e2e612afa1c6ebf8fc208e45ffd
    overlay-script: |
      sed -i 's/http:\/\/deb.\(.*\)/https:\/\/deb.\1/g' /etc/apt/sources.list
      apt-get update
      ln -svf /lib /lib64
      apt-get install -y bash
      mkdir -p /opt/spark/python
      touch /opt/spark/RELEASE
      rm /bin/sh
      ln -svf /bin/bash /bin/sh
      echo "auth required pam_wheel.so use_uid" >> /etc/pam.d/su
      rm -rf /var/cache/apt/*
    organize:
      conf: etc/spark/conf
      jars: opt/spark/jars
      bin: opt/spark/bin
      sbin: opt/spark/sbin
      python: opt/spark/python
      kubernetes/dockerfiles/spark/entrypoint.sh: opt/spark/entrypoint.sh
      kubernetes/dockerfiles/spark/decom.sh: opt/spark/decom.sh
      examples: opt/spark/examples
      kubernetes/tests: opt/spark/tests
      data: opt/spark/data
    stage:
      - etc/spark/conf
      - opt/spark/jars
      - opt/spark/bin
      - opt/spark/sbin
      - opt/spark/entrypoint.sh
      - opt/spark/decom.sh
      - opt/spark/examples
      - opt/spark/tests
      - opt/spark/python
      - opt/spark/data

  kyuubi:
    plugin: dump
    after: [ spark ]
    source: https://dlcdn.apache.org/kyuubi/kyuubi-1.8.1/apache-kyuubi-1.8.1-bin.tgz
    source-checksum: sha512/355ebbc184bbb0ab2fe521dda213a7769dae222a6a278a1fc0b3bf1173fd26091ccbc073038564ea8bb2ad2d5f5d1049c19508201ed710978d2ea79d20cd5726
    organize:
      beeline-jars: opt/kyuubi/beeline-jars
      bin: opt/kyuubi/bin
      charts: opt/kyuubi/charts
      conf: opt/kyuubi/conf
      db-scripts: opt/kyuubi/db-scripts
      docker: opt/kyuubi/docker
      externals: opt/kyuubi/externals
      jars: opt/kyuubi/jars
      logs: opt/kyuubi/logs
      pid: opt/kyuubi/pid
      web-ui: opt/kyuubi/web-ui
      work: opt/kyuubi/work
    stage:
      - opt/kyuubi/beeline-jars
      - opt/kyuubi/bin
      - opt/kyuubi/charts
      - opt/kyuubi/conf
      - opt/kyuubi/db-scripts
      - opt/kyuubi/docker
      - opt/kyuubi/externals
      - opt/kyuubi/jars
      - opt/kyuubi/logs
      - opt/kyuubi/pid
      - opt/kyuubi/web-ui
      - opt/kyuubi/work

  dependencies:
    plugin: nil
    after: [ kyuubi ]
    build-packages:
      - wget
    overlay-script: |
      AWS_JAVA_SDK_BUNDLE_VERSION='1.12.540'
      HADOOP_AWS_VERSION='3.3.6'
      ICEBERG_SPARK_RUNTIME_VERSION='3.4_2.12'
      ICEBERG_VERSION='1.4.3'
      SHA1SUM_AWS_JAVA_SDK_BUNDLE_JAR='a351ebc4f81d20e0349b3c0f85f34f443a37ce9d'
      SHA1SUM_HADOOP_AWS_JAR='d5e162564701848b0921b80aedef9e64435333cc'
      SHA512SUM_SPARK_METRICS_ASSEMBLY_JAR='fc52ba79af46e008b1463da4c0852564f2bfce21668468b683550df1f1ff3e4f149641bbce1cffb24510569c1a441bb47f73dd2b0cff87073631c391dc248211'
      SHA1SUM_ICEBERG_JAR='48d553e4e5496f731b9e0e6adb5bc0fd040cb0df'
      mkdir -p $CRAFT_PART_INSTALL/opt/spark/jars
      cd $CRAFT_PART_INSTALL/opt/spark/jars
      wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar"
      echo "${SHA1SUM_AWS_JAVA_SDK_BUNDLE_JAR} aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar"
      echo "${SHA1SUM_HADOOP_AWS_JAR}  hadoop-aws-${HADOOP_AWS_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: hadoop-aws-${HADOOP_AWS_VERSION}.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      wget -q "https://github.com/canonical/central-uploader/releases/download/spark-metrics-assembly-3.4-1.0.0/spark-metrics-assembly-3.4-1.0.0.jar"
      wget -q "https://github.com/canonical/central-uploader/releases/download/spark-metrics-assembly-3.4-1.0.0/spark-metrics-assembly-3.4-1.0.0.jar.sha512"
      echo "${SHA512SUM_SPARK_METRICS_ASSEMBLY_JAR} spark-metrics-assembly-3.4-1.0.0.jar" | sha512sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: spark-metrics-assembly-3.4-1.0.0.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      wget -q "https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-${ICEBERG_SPARK_RUNTIME_VERSION}/${ICEBERG_VERSION}/iceberg-spark-runtime-${ICEBERG_SPARK_RUNTIME_VERSION}-${ICEBERG_VERSION}.jar"
      echo "${SHA1SUM_ICEBERG_JAR} iceberg-spark-runtime-${ICEBERG_SPARK_RUNTIME_VERSION}-${ICEBERG_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: iceberg-spark-runtime-${ICEBERG_SPARK_RUNTIME_VERSION}-${ICEBERG_VERSION}.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
    stage:
      - opt/spark/jars

  spark8t:
    plugin: nil
    after: [ dependencies ]
    build-packages:
      - wget
      - ssl-cert
      - git
    overlay-packages:
      - python3-pip
    overlay-script: |
      mkdir -p $CRAFT_PART_INSTALL/opt/spark8t/python/dist
      pip install --target=${CRAFT_PART_INSTALL}/opt/spark8t/python/dist  https://github.com/canonical/spark-k8s-toolkit-py/releases/download/v0.0.5/spark8t-0.0.5-py3-none-any.whl
      rm usr/bin/pip*
    stage:
      - opt/spark8t/python/dist

  kubectl:
    plugin: nil
    build-packages:
      - wget
    overlay-script: |
      mkdir -p $CRAFT_PART_INSTALL/usr/local/bin
      cd $CRAFT_PART_INSTALL/usr/local/bin
      KUBECTL_VERSION=$(wget -qO- https://dl.k8s.io/release/stable.txt)
      wget -q "https://dl.k8s.io/release/$KUBECTL_VERSION/bin/linux/amd64/kubectl"
      wget -q  "https://dl.k8s.io/$KUBECTL_VERSION/bin/linux/amd64/kubectl.sha256"
      echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: kubectl could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      chmod +x kubectl
      cp -r $CRAFT_PART_INSTALL/usr/ $CRAFT_PART_BUILD
    stage:
      - usr/local/bin

  charmed-spark:
    plugin: dump
    after: [ spark8t, spark ]
    source: files/spark
    organize:
      conf/spark-defaults.conf: etc/spark8t/conf/spark-defaults.conf
      bin/sparkd.sh: opt/pebble/sparkd.sh
      bin/history-server.sh: opt/pebble/history-server.sh
      bin/thrift-server.sh: opt/pebble/thrift-server.sh
      bin/spark-client.pyspark: opt/spark-client/python/bin/spark-client.pyspark
      bin/spark-client.spark-sql: opt/spark-client/python/bin/spark-client.spark-sql
      bin/spark-client.service-account-registry: opt/spark-client/python/bin/spark-client.service-account-registry
      bin/spark-client.spark-shell: opt/spark-client/python/bin/spark-client.spark-shell
      bin/spark-client.spark-submit: opt/spark-client/python/bin/spark-client.spark-submit
    stage:
      - etc/spark8t/conf/
      - opt/pebble/sparkd.sh
      - opt/pebble/history-server.sh
      - opt/pebble/thrift-server.sh
      - opt/spark-client/python/bin/spark-client.pyspark
      - opt/spark-client/python/bin/spark-client.spark-sql
      - opt/spark-client/python/bin/spark-client.service-account-registry
      - opt/spark-client/python/bin/spark-client.spark-shell
      - opt/spark-client/python/bin/spark-client.spark-submit

  user-setup:
    plugin: nil
    after: [ charmed-spark ]
    overlay-packages:
      - tini
      - libc6
      - libpam-modules
      - krb5-user
      - libnss3
      - procps
      - openjdk-11-jre-headless

    override-prime: |
      # Please refer to https://discourse.ubuntu.com/t/unifying-user-identity-across-snaps-and-rocks/36469
      # for more information about shared user.
      SPARK_GID=584792
      SPARK_UID=584792

      craftctl default
      chmod 755 opt/spark-client/python/bin/*

      chown -R ${SPARK_GID}:${SPARK_UID} etc/spark etc/spark8t
      chmod -R 750 etc/spark etc/spark8t

      mkdir -p var/log/spark
      chown -R ${SPARK_GID}:${SPARK_UID} var/log/spark
      chmod -R 750 var/log/spark

      # This is needed to run the spark job, as it requires RW+ access on the spark folder
      chown -R ${SPARK_GID}:${SPARK_UID} opt/spark
      chmod -R 750 opt/spark

      chown -R ${SPARK_GID}:${SPARK_UID} opt/kyuubi
      chmod -R 750 opt/kyuubi

      mkdir -p var/lib/spark
      mkdir -p var/lib/spark/notebook
      chown -R ${SPARK_GID}:${SPARK_UID} var/lib/spark
      chmod -R 770 var/lib/spark
      mv opt/spark/decom.sh opt/decom.sh
      chmod a+x opt/decom.sh
