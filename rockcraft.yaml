name: charmed-spark
summary: Spark ROCK
description: Spark ROCK
license: Apache-2.0

version: "3.4.0"
base: ubuntu:22.04
platforms:
  amd64:

entrypoint: ["/bin/bash", "/opt/pebble-start.sh"]

env:
  - SPARK_HOME: /opt/spark
  - JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64
  - PYTHONPATH: /opt/spark/python:/opt/spark/python/dist:/usr/lib/python3/dist-packages
  - PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/spark:/opt/spark/bin:/opt/spark/python/bin
  - HOME: /home/spark
  - KUBECONFIG: /home/spark/.kube/config
  - SPARK_USER_DATA: opt/spark/data

parts:

  spark:
    plugin: dump
    source: https://downloads.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz
    source-checksum: sha512/67bc912e9192ef2159540cb480820e5466dfd91e907c97c5a4787587e3020be042b76c40c51854f2a5dbeb8c3775fe12d9021c1200c4704463ec644132243a69
    overlay-script: |
      set -ex
      sed -i 's/http:\/\/deb.\(.*\)/https:\/\/deb.\1/g' /etc/apt/sources.list
      apt-get update
      ln -svf /lib /lib64
      apt-get install -y bash
      mkdir -p /opt/spark/python
      touch /opt/spark/RELEASE
      rm /bin/sh
      ln -svf /bin/bash /bin/sh
      echo "auth required pam_wheel.so use_uid" >> /etc/pam.d/su
      rm -rf /var/cache/apt/*
    organize:
      conf: opt/spark/conf
      jars: opt/spark/jars
      bin: opt/spark/bin
      sbin: opt/spark/sbin
      python: opt/spark/python
      kubernetes/dockerfiles/spark/entrypoint.sh: opt/entrypoint.sh
      kubernetes/dockerfiles/spark/decom.sh: opt/decom.sh
      examples: opt/spark/examples
      kubernetes/tests: opt/spark/tests
      data: opt/spark/data
    stage:
      - opt/spark/conf
      - opt/spark/jars
      - opt/spark/bin
      - opt/spark/sbin
      - opt/entrypoint.sh
      - opt/decom.sh
      - opt/spark/examples
      - opt/spark/tests
      - opt/spark/data
      - opt/spark/python

  hadoop-jars:
    plugin: nil
    after: [spark]
    build-packages:
      - wget
    overlay-script: |
      AWS_JAVA_SDK_BUNDLE_VERSION='1.11.874'
      HADOOP_AWS_VERSION='3.2.2'
      mkdir -p $CRAFT_PART_INSTALL/opt/spark/jars
      cd $CRAFT_PART_INSTALL/opt/spark/jars
      wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar"
      wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar.sha1"
      echo "`cat aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar.sha1`  aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar"
      wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar.sha1"
      echo "`cat hadoop-aws-${HADOOP_AWS_VERSION}.jar.sha1`  hadoop-aws-${HADOOP_AWS_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: hadoop-aws-${HADOOP_AWS_VERSION}.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
    stage:
      - opt/spark/jars

  client-scripts:
    plugin: nil
    after: [hadoop-jars]
    build-packages:
      - wget
      - ssl-cert
      - git
    overlay-packages:
      - python3-pip
    overlay-script: |
      mkdir -p $CRAFT_PART_INSTALL/opt/spark/python/dist
      pip install --target=${CRAFT_PART_INSTALL}/opt/spark/python/dist git+https://github.com/canonical/spark-k8s-toolkit-py.git@main
    stage:
      - opt/spark/python/dist

  kubectl-bin:
    plugin: nil
    after: [client-scripts]
    build-packages:
      - wget
    overlay-script: |
      mkdir -p $CRAFT_PART_INSTALL/usr/local/bin
      cd $CRAFT_PART_INSTALL/usr/local/bin
      KUBECTL_VERSION=$(wget -qO- https://dl.k8s.io/release/stable.txt)
      wget -q "https://dl.k8s.io/release/$KUBECTL_VERSION/bin/linux/amd64/kubectl"
      wget -q  "https://dl.k8s.io/$KUBECTL_VERSION/bin/linux/amd64/kubectl.sha256"
      echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: kubectl could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      chmod +x kubectl
    stage:
      - usr/local/bin

  spark-conf:
    plugin: dump
    after: [kubectl-bin]
    source: files/spark/conf
    organize:
      spark-defaults.conf: opt/spark/conf/spark-defaults.conf
    stage:
      - opt/spark/conf/spark-defaults.conf

  user-setup:
    plugin: nil
    after: [spark-conf]
    overlay-packages:
      - tini
      - libc6
      - libpam-modules
      - krb5-user
      - libnss3
      - procps
      - openjdk-11-jre-headless
      - python3-setuptools
    overlay-script: |
      SPARK_GID=185
      SPARK_UID=185

      # Create a user in the $CRAFT_OVERLAY chroot
      groupadd -R $CRAFT_OVERLAY -g ${SPARK_GID} spark
      useradd -R $CRAFT_OVERLAY -m -r -g spark -u ${SPARK_UID} spark
    override-prime: |
      SPARK_GID=185
      SPARK_UID=185

      craftctl default
      chown -R ${SPARK_GID}:${SPARK_UID} opt/spark
      chmod -R 770 opt/spark

      chown ${SPARK_GID}:${SPARK_UID} opt/entrypoint.sh
      chmod 770 opt/entrypoint.sh

      chown ${SPARK_GID}:${SPARK_UID} opt/decom.sh
      chmod 770 opt/decom.sh

      chown -R ${SPARK_GID}:${SPARK_UID} home/spark
      chmod -R 770 home/spark

  pebble-setup:
    plugin: dump
    source: files/pebble
    organize:
      pebble-start.sh: opt/pebble-start.sh
    overlay-script:
      mkdir -p $CRAFT_PART_INSTALL/var/lib/pebble/default
    stage:
      - opt/pebble-start.sh
      - var/lib/pebble/default
