name: charmed-spark
summary: Spark ROCK
description: Spark ROCK
license: Apache-2.0

version: "3.4.1"
base: ubuntu:22.04
platforms:
  amd64:

run_user: _daemon_

environment:
  SPARK_HOME: /opt/spark
  SPARK_CONFS: /etc/spark8t/conf
  JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64
  PYTHONPATH: /opt/spark/python:/opt/spark8t/python/dist:/usr/lib/python3.10/site-packages
  PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/spark:/opt/spark/bin:/opt/spark/python/bin:/opt/spark-client/python/bin
  HOME: /var/lib/spark
  KUBECONFIG: /var/lib/spark/.kube/config
  SPARK_USER_DATA: /var/lib/spark
  SPARK_LOG_DIR: /var/log/spark

services:
  entrypoint:
    command: "/bin/bash /opt/pebble/charmed-spark-entrypoint.sh"
    summary: "This is the service to startup Spark processes using the Spark entrypoint"
    override: replace
    startup: enabled
    on-success: shutdown
    on-failure: shutdown
  history-server:
    command: "/bin/bash /opt/pebble/charmed-spark-history-server.sh"
    summary: "This is the Spark History Server service"
    override: replace
    startup: disabled
    # Not working in charms
    # working-dir: /opt/spark
    environment:
      SPARK_PROPERTIES_FILE: /etc/spark8t/conf/spark-defaults.conf
  thrift-server:
    command: "/bin/bash /opt/pebble/charmed-spark-thrift-server.sh"
    summary: "This is the Spark Thrift Server service"
    override: replace
    startup: disabled
    environment:
      SPARK_PROPERTIES_FILE: /etc/spark8t/conf/spark-defaults.conf
  jupyter:
    command: "spark-client.pyspark [ --username spark --namespace spark ]"
    summary: "This is the Spark-powered Jupyter service"
    override: replace
    startup: disabled
    environment:
      PYSPARK_DRIVER_PYTHON: jupyter
      PYSPARK_DRIVER_PYTHON_OPTS: "lab --no-browser --port=8888 --ip=0.0.0.0 --NotebookApp.token='' --notebook-dir=/var/lib/spark/notebook"

parts:
  spark:
    plugin: dump
    source: https://github.com/canonical/central-uploader/releases/download/spark-3.4.1-ubuntu1/spark-3.4.1-ubuntu1-20231004201041-bin-k8s.tgz
    source-checksum: sha512/c229e0e1813873868368733f73138c47c70153e0a612823971e8b6a3ed79358ac3514e552b14c5aa579e1e0feed0dfdc672ab87b1d46e944efedb45ca538d2df
    overlay-script: |
      sed -i 's/http:\/\/deb.\(.*\)/https:\/\/deb.\1/g' /etc/apt/sources.list
      apt-get update
      ln -svf /lib /lib64
      apt-get install -y bash
      mkdir -p /opt/spark/python
      touch /opt/spark/RELEASE
      rm /bin/sh
      ln -svf /bin/bash /bin/sh
      echo "auth required pam_wheel.so use_uid" >> /etc/pam.d/su
      rm -rf /var/cache/apt/*
    organize:
      conf: etc/spark/conf
      jars: opt/spark/jars
      bin: opt/spark/bin
      sbin: opt/spark/sbin
      python: opt/spark/python
      kubernetes/dockerfiles/spark/entrypoint.sh: opt/spark/entrypoint.sh
      kubernetes/dockerfiles/spark/decom.sh: opt/spark/decom.sh
      examples: opt/spark/examples
      kubernetes/tests: opt/spark/tests
      data: opt/spark/data
    stage:
      - etc/spark/conf
      - opt/spark/jars
      - opt/spark/bin
      - opt/spark/sbin
      - opt/spark/entrypoint.sh
      - opt/spark/decom.sh
      - opt/spark/examples
      - opt/spark/tests
      - opt/spark/python
      - opt/spark/data

  dependencies:
    plugin: nil
    after: [ spark ]
    build-packages:
      - wget
    overlay-script: |
      AWS_JAVA_SDK_BUNDLE_VERSION='1.12.540'
      HADOOP_AWS_VERSION='3.3.6'
      mkdir -p $CRAFT_PART_INSTALL/opt/spark/jars
      cd $CRAFT_PART_INSTALL/opt/spark/jars
      wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar"
      wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar.sha1"
      echo "`cat aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar.sha1`  aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar"
      wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar.sha1"
      echo "`cat hadoop-aws-${HADOOP_AWS_VERSION}.jar.sha1`  hadoop-aws-${HADOOP_AWS_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: hadoop-aws-${HADOOP_AWS_VERSION}.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      wget -q "https://github.com/canonical/central-uploader/releases/download/spark-metrics-assembly-3.4-1.0.0/spark-metrics-assembly-3.4-1.0.0.jar"
      wget -q "https://github.com/canonical/central-uploader/releases/download/spark-metrics-assembly-3.4-1.0.0/spark-metrics-assembly-3.4-1.0.0.jar.sha512"
      echo "`cat spark-metrics-assembly-3.4-1.0.0.jar.sha512`" | sha512sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: spark-metrics-assembly-3.4-1.0.0.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
    stage:
      - opt/spark/jars

  spark8t:
    plugin: nil
    after: [ dependencies ]
    build-packages:
      - wget
      - ssl-cert
      - git
    overlay-packages:
      - python3-pip
    overlay-script: |
      mkdir -p $CRAFT_PART_INSTALL/opt/spark8t/python/dist
      pip install --target=${CRAFT_PART_INSTALL}/opt/spark8t/python/dist  https://github.com/canonical/spark-k8s-toolkit-py/releases/download/v0.0.1/spark8t-0.0.1-py3-none-any.whl
      rm usr/bin/pip*
    stage:
      - opt/spark8t/python/dist

  jupyter:
    plugin: python
    source: .
    python-packages:
      - jupyterlab
    stage-packages:
      - python3-venv
    organize:
      lib: usr/lib
      bin: usr/bin
      share: usr/share
    stage:
      - usr

  kubectl:
    plugin: nil
    build-packages:
      - wget
    overlay-script: |
      mkdir -p $CRAFT_PART_INSTALL/usr/local/bin
      cd $CRAFT_PART_INSTALL/usr/local/bin
      KUBECTL_VERSION=$(wget -qO- https://dl.k8s.io/release/stable.txt)
      wget -q "https://dl.k8s.io/release/$KUBECTL_VERSION/bin/linux/amd64/kubectl"
      wget -q  "https://dl.k8s.io/$KUBECTL_VERSION/bin/linux/amd64/kubectl.sha256"
      echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: kubectl could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      chmod +x kubectl
      cp -r $CRAFT_PART_INSTALL/usr/ $CRAFT_PART_BUILD
    stage:
      - usr/local/bin

  charmed-spark:
    plugin: dump
    after: [ spark8t, spark ]
    source: files/spark
    organize:
      conf/spark-defaults.conf: etc/spark8t/conf/spark-defaults.conf
      bin/charmed-spark-entrypoint.sh: opt/pebble/charmed-spark-entrypoint.sh
      bin/charmed-spark-history-server.sh: opt/pebble/charmed-spark-history-server.sh
      bin/charmed-spark-thrift-server.sh: opt/pebble/charmed-spark-thrift-server.sh
      bin/spark-client.pyspark: opt/spark-client/python/bin/spark-client.pyspark
      bin/spark-client.service-account-registry: opt/spark-client/python/bin/spark-client.service-account-registry
      bin/spark-client.spark-shell: opt/spark-client/python/bin/spark-client.spark-shell
      bin/spark-client.spark-submit: opt/spark-client/python/bin/spark-client.spark-submit
    stage:
      - etc/spark8t/conf/
      - opt/pebble/charmed-spark-entrypoint.sh
      - opt/pebble/charmed-spark-history-server.sh
      - opt/pebble/charmed-spark-thrift-server.sh
      - opt/spark-client/python/bin/spark-client.pyspark
      - opt/spark-client/python/bin/spark-client.service-account-registry
      - opt/spark-client/python/bin/spark-client.spark-shell
      - opt/spark-client/python/bin/spark-client.spark-submit

  user-setup:
    plugin: nil
    after: [ charmed-spark ]
    overlay-packages:
      - tini
      - libc6
      - libpam-modules
      - krb5-user
      - libnss3
      - procps
      - openjdk-11-jre-headless

    override-prime: |
      # Please refer to https://discourse.ubuntu.com/t/unifying-user-identity-across-snaps-and-rocks/36469
      # for more information about shared user.
      SPARK_GID=584792
      SPARK_UID=584792

      craftctl default
      chmod 755 opt/spark-client/python/bin/*

      chown -R ${SPARK_GID}:${SPARK_UID} etc/spark etc/spark8t
      chmod -R 750 etc/spark etc/spark8t

      mkdir -p var/log/spark
      chown -R ${SPARK_GID}:${SPARK_UID} var/log/spark
      chmod -R 750 var/log/spark

      # This is needed to run the spark job, as it requires RW+ access on the spark folder
      chown -R ${SPARK_GID}:${SPARK_UID} opt/spark
      chmod -R 750 opt/spark

      mkdir -p var/lib/spark
      mkdir -p var/lib/spark/notebook
      chown -R ${SPARK_GID}:${SPARK_UID} var/lib/spark
      chmod -R 770 var/lib/spark
