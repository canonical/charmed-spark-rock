name: charmed-spark
summary: Spark ROCK
description: Spark ROCK
license: Apache-2.0

version: "3.4.1"
base: ubuntu:22.04
platforms:
  amd64:

run_user: _daemon_

environment:
  SPARK_HOME: /opt/spark
  SPARK_CONFS: /etc/spark8t/conf
  JAVA_HOME: /usr/lib/jvm/java-11-openjdk-amd64
  PYTHONPATH: /opt/spark/python:/opt/spark8t/python/dist:/usr/lib/python3/dist-packages
  PATH: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/spark:/opt/spark/bin:/opt/spark/python/bin:/opt/spark-client/python/bin
  HOME: /home/spark
  KUBECONFIG: /home/spark/.kube/config
  SPARK_USER_DATA: /home/spark
  SPARK_LOG_DIR: /var/log/spark

services:
  entrypoint:
    command: "/bin/bash /opt/pebble/charmed-spark-entrypoint.sh"
    summary: "This is the service to startup Spark processes using the Spark entrypoint"
    override: replace
    startup: enabled
    on-success: shutdown
    on-failure: shutdown
  history-server:
    command: "/bin/bash /opt/pebble/charmed-spark-history-server.sh"
    summary: "This is the Spark History Server service"
    override: replace
    startup: disabled
    # Not working in charms
    # working-dir: /opt/spark
    environment:
      SPARK_PROPERTIES_FILE: /etc/spark8t/conf/spark-defaults.conf

parts:
  spark:
    plugin: dump
    source: https://downloads.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
    source-checksum: sha512/5a21295b4c3d1d3f8fc85375c711c7c23e3eeb3ec9ea91778f149d8d321e3905e2f44cf19c69a28df693cffd536f7316706c78932e7e148d224424150f18b2c5
    overlay-script: |
      sed -i 's/http:\/\/deb.\(.*\)/https:\/\/deb.\1/g' /etc/apt/sources.list
      apt-get update
      ln -svf /lib /lib64
      apt-get install -y bash
      mkdir -p /opt/spark/python
      touch /opt/spark/RELEASE
      rm /bin/sh
      ln -svf /bin/bash /bin/sh
      echo "auth required pam_wheel.so use_uid" >> /etc/pam.d/su
      rm -rf /var/cache/apt/*
    organize:
      conf: etc/spark/conf
      jars: opt/spark/jars
      bin: opt/spark/bin
      sbin: opt/spark/sbin
      python: opt/spark/python
      kubernetes/dockerfiles/spark/entrypoint.sh: opt/spark/entrypoint.sh
      kubernetes/dockerfiles/spark/decom.sh: opt/spark/decom.sh
      examples: opt/spark/examples
      kubernetes/tests: opt/spark/tests
      data: opt/spark/data
    stage:
      - etc/spark/conf
      - opt/spark/jars
      - opt/spark/bin
      - opt/spark/sbin
      - opt/spark/entrypoint.sh
      - opt/spark/decom.sh
      - opt/spark/examples
      - opt/spark/tests
      - opt/spark/python
      - opt/spark/data

  hadoop:
    plugin: nil
    after: [ spark ]
    build-packages:
      - wget
    overlay-script: |
      AWS_JAVA_SDK_BUNDLE_VERSION='1.12.540'
      HADOOP_AWS_VERSION='3.3.6'
      mkdir -p $CRAFT_PART_INSTALL/opt/spark/jars
      cd $CRAFT_PART_INSTALL/opt/spark/jars
      wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar"
      wget -q "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_JAVA_SDK_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar.sha1"
      echo "`cat aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar.sha1`  aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: aws-java-sdk-bundle-${AWS_JAVA_SDK_BUNDLE_VERSION}.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar"
      wget -q "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar.sha1"
      echo "`cat hadoop-aws-${HADOOP_AWS_VERSION}.jar.sha1`  hadoop-aws-${HADOOP_AWS_VERSION}.jar" | sha1sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: hadoop-aws-${HADOOP_AWS_VERSION}.jar could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
    stage:
      - opt/spark/jars

  spark8t:
    plugin: nil
    after: [ hadoop ]
    build-packages:
      - wget
      - ssl-cert
      - git
    overlay-packages:
      - python3-pip
    overlay-script: |
      mkdir -p $CRAFT_PART_INSTALL/opt/spark8t/python/dist
      pip install --target=${CRAFT_PART_INSTALL}/opt/spark8t/python/dist  https://github.com/canonical/spark-k8s-toolkit-py/releases/download/v0.0.1/spark8t-0.0.1-py3-none-any.whl
    stage:
      - opt/spark8t/python/dist

  kubectl:
    plugin: nil
    build-packages:
      - wget
    overlay-script: |
      mkdir -p $CRAFT_PART_INSTALL/usr/local/bin
      cd $CRAFT_PART_INSTALL/usr/local/bin
      KUBECTL_VERSION=$(wget -qO- https://dl.k8s.io/release/stable.txt)
      wget -q "https://dl.k8s.io/release/$KUBECTL_VERSION/bin/linux/amd64/kubectl"
      wget -q  "https://dl.k8s.io/$KUBECTL_VERSION/bin/linux/amd64/kubectl.sha256"
      echo "$(cat kubectl.sha256)  kubectl" | sha256sum --check
      if  [[ $? -ne 0 ]]
        then
          echo "DOWNLOAD ERROR: kubectl could not be downloaded properly! Exiting...." >&2
          exit 1
      fi
      chmod +x kubectl
      cp -r $CRAFT_PART_INSTALL/usr/ $CRAFT_PART_BUILD
    stage:
      - usr/local/bin

  charmed-spark:
    plugin: dump
    after: [ spark8t, spark ]
    source: files/spark
    organize:
      conf/spark-defaults.conf: etc/spark8t/conf/spark-defaults.conf
      bin/charmed-spark-entrypoint.sh: opt/pebble/charmed-spark-entrypoint.sh
      bin/charmed-spark-history-server.sh: opt/pebble/charmed-spark-history-server.sh
      bin/spark-client.pyspark: opt/spark-client/python/bin/spark-client.pyspark
      bin/spark-client.service-account-registry: opt/spark-client/python/bin/spark-client.service-account-registry
      bin/spark-client.spark-shell: opt/spark-client/python/bin/spark-client.spark-shell
      bin/spark-client.spark-submit: opt/spark-client/python/bin/spark-client.spark-submit
    stage:
      - etc/spark8t/conf/
      - opt/pebble/charmed-spark-entrypoint.sh
      - opt/pebble/charmed-spark-history-server.sh
      - opt/spark-client/python/bin/spark-client.pyspark
      - opt/spark-client/python/bin/spark-client.service-account-registry
      - opt/spark-client/python/bin/spark-client.spark-shell
      - opt/spark-client/python/bin/spark-client.spark-submit

  user-setup:
    plugin: nil
    after: [ charmed-spark ]
    overlay-packages:
      - tini
      - libc6
      - libpam-modules
      - krb5-user
      - libnss3
      - procps
      - openjdk-11-jre-headless
      - python3-setuptools

    override-prime: |
      SPARK_GID=584792
      SPARK_UID=584792

      craftctl default
      chmod 755 opt/spark-client/python/bin/*

      chown -R ${SPARK_GID}:${SPARK_UID} etc
      chmod -R 770 etc

      chown -R ${SPARK_GID}:${SPARK_UID} var
      chmod -R 770 var

      # This is needed to run the spark job, as it requires RW+ access on the spark folder
      chown -R ${SPARK_GID}:${SPARK_UID} opt/spark
      chmod -R 770 var

      mkdir -p home/spark
      chown -R ${SPARK_GID}:${SPARK_UID} home/spark
      chmod -R 770 home/spark

      mkdir -p var/log/spark
      chown -R ${SPARK_GID}:${SPARK_UID} var/log/spark
      chmod -R 770 var/log/spark

